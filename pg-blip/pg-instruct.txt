AIが「見た目に対する物理的感覚」を得ました。
”変形しにくい容器はどっち？”
"壊れやすいのはどっち？"
などの問いに対して正確に答えます。

スタンフォード大学、DeepMind、プリンストン大学の研究チームが報告しました。

○ Jensen Gao et al. Physically Grounded Vision-Language Models for Robotic Manipulation

研究者たちはAIに物理的概念を教える『PhysObjects』という新しいデータセットを作成し、訓練されたモデルが驚きのタスクに成功する結果に出会いました。

■『PhysObjects』の内容
約36,900個の家庭用品に対する、約417,000件の「物理概念」（※）ラベルデータ

※「物理概念」の内訳
・質量（Mass）
・壊れやすさ（Fragility）
・変形性（Deformability）
・材料（Material）
・透明性（Transparency）
・内容物（Contents）
・液体の保持能力（Can Contain Liquid）
・密封性（Is Sealed）

■『PhysObjects』とVLMの使い方
① PhysObjectsでVLMを微調整
② VLMの「オブジェクトに対する物理的感覚」が向上
③ VLMが「見た目から物理情報を推測できる」ようになる
④ VLMがロボットなどに組み込まれる

■実験結果
PhysObjectsで微調整されたVLMが組み込まれたロボティックマニピュレーションが、以下などのタスクに成功しました。
・水を運ぶことができる容器のみ移動する
・最も壊れやすいオブジェクトを移動する
・最も質量が少ない2つのオブジェクトを、最も変形しにくい容器に入れる
・最も重い3つのオブジェクトを移動する

なお、データセットやモデルは公開されており、誰でも試すことができます。
